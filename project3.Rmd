---
title: "Project3"
author: "Group 6"
date: "4/2/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(MASS)
library(BART)
library(kernlab)
library(class)
```


```{r}
absent <-read.csv("Absenteeism_at_work2.csv")
str(absent)
```

```{r}
##Converting into factors
#absent$ï..ID<- as.factor(absent$ï..ID)
absent$Reason.for.absence <- as.factor(absent$Reason.for.absence)
absent$Month.of.absence <- as.factor(absent$Month.of.absence)
absent$Day.of.the.week <- as.factor(absent$Day.of.the.week)
absent$Seasons <- as.factor(absent$Season)
absent$Disciplinary.failure <-as.factor(absent$Disciplinary.failure)
absent$Education <- as.factor(absent$Education)
absent$Social.drinker <-as.factor(absent$Social.drinker)
absent$Social.smoker <-as.factor(absent$Social.smoker)
absent$ï..ID  <- NULL
absent$Work.load.Average.day<-as.numeric(absent$Work.load.Average.day)
absent$Month.of.absence<- NULL
absent$Weight<- NULL
absent$is_Thur<-if_else(absent$Day.of.the.week==5,1,0)
absent$Day.of.the.week<-NULL
absent$is_Seas4<-if_else(absent$Seasons==4,1,0)
absent$Seasons<-NULL
#absent$is_Educ3<-if_else(absent$Education==3,1,0) Once I do this, Seasons and Education3 no longer significant
#absent$Education<-NULL
absent$Transportation.expense<-NULL
absent$Social.drinker<-NULL
absent$Disciplinary.failure<-NULL
```

```{r}
lm.model <- lm( Absenteeism.time.in.hours ~., data=absent)
stepmodel <- step(lm.model, direction = "forward")
summary(stepmodel)

lm1 <-lm(Absenteeism.time.in.hours ~.,data=absent) %>% stepAIC(trace = FALSE, direction = "forward")
summary(lm1)
# ANN/Decision tree/Logistic Rminer?
```

# ANN/Decision tree/Logistic Rminer?
Reason for absense: Overall reasons for absence are significant, but some reasons have more of impact than others. I guess that's because some reasons are commonplace or popular while others are more personal and not widely used.

Month of absence:It is not a significant factor, however by common sense, we would expect people to take more leaves during holiday season,such as Christams and New Year Holiday,and take less leaves after that, which is the first quarter of the year.

Day of the Week: Comparatively, people take more leaves in the first few days of the week, one possible explanation is people tend to have "Monday Syndrome", and in the last few workdays(Thursdays and Fridays) they tend to work harder and longer to finish the work in time in order to have a good weekend.

Seasons: Similar to Month of absense. We can see people take longer leaves during season 2,3,4 than season 1, I guess that's because in season 1 most people just come back from Christmas and New Year Holiday, and there are less holidays in season 1.

Transportation expense: We expected that people who need more transportation expenses tend to have more absense. Interestingly, the coefficient is postive but really small, also it is not significant. I guess this is because most people won't consider transportation expenses when they are thinking if they want to go to work today (since you can make more money than transportation expenses if you work)

Distance from Residence to Work: The result is kind of counterintuitive since we expectd that people who liver closer to the company will show up more compared to people who don't. Though it is not significant, the relationship is negative. Maybe it is becuase people who liver closer tend to procrastinate and leave home later than people who live farther away and get up earlier.

Service time:Intuitively, people who do more volunteer work would of course ask for more leaves than people who don't.

Age:Older people may ask for more leaves due to health issues, or because they are more senior so the legwork can be handed to juniors and they can leave earlier.

Work load Average day:The coefficient is negative, which is straigtforward because the more workload you have the less likely you will take a break.

Hit target:If you have hit the target of your manager/your team, you can be rewarded with more breaks.

Education:The higher degree you have, the more capable you are and therefore, you will need to handle more challenging work that may require more time.

Son:It is pretty straightforward that more kids means people need more commitment to the family and more parental leaves, therefore the relationship is negative.

Social drinker: ?

Social smoker: ?

Pet:We expected it to be similar to "Son", however the relationship is negative. One possible interpreation is that people who have pets are mostly young people who don't have kids. Also, pets need less dedication that kids(since you don't need to cook for pets and tutor them)


Weight, Height, Body mass index:We expected a healthy body would save people from taking sick leaves. Weight and height are very insignificant since people with different heights have different healthy weights. BMI index is also significant since a good BMI is between 18.5-24.9——neither a too high or too low BMI index is good.


Preparing the data
```{r}
##Building categories
absent$ID <-NULL
absent$timecat <- absent$Absenteeism.time.in.hours
absent$timecat <- ifelse(absent$timecat == 0, "not.absent", ifelse (absent$timecat>0 & absent$timecat<8, "less.than.day", ifelse(absent$timecat>=8 & absent$timecat<=56, "one.to.seven.days", "more.than.one.week")))
table(absent$timecat)
          
absent$timecat <- as.factor(absent$timecat)
absent$Absenteeism.time.in.hours <- NULL


##Randomizing the data
set.seed(65)
absent_r<-absent[sample(nrow(absent)),]

##Turning factors into binary
absent_mm <- as.data.frame(model.matrix(~ . -1, absent_r[-15])) ##Breaking factors into dummy variables


normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

##Normalizing every variable except output
absent_n <- as.data.frame(lapply(absent_mm, normalize))


```

##KNN Model
```{r}
##Getting my own absent from the above
absent_SP <- cbind(absent_n, absent$timecat)

##Rebulding the test and train data
abs_train <- absent_SP[1:540, -44]
abs_test <- absent_SP[541:740,-44]
abs_train_label <- absent_SP[1:540, 44]
abs_test_label <- absent_SP[541:740,44]

##building KNN model
KNN_MODEL <- knn(train = abs_train, test = abs_test,
                      cl = abs_train_label, k=40)

confusionMatrix(as.factor(KNN_MODEL),as.factor(abs_test_label))
```


## ANN Model
```{r}

#Dividing response variable into categories 
absent$timecat <- absent$Absenteeism.time.in.hours
absent$timecat <- ifelse(absent$timecat == 0, "not.absent", ifelse (absent$timecat>0 & absent$timecat<8, "less.than.day", ifelse(absent$timecat>=8 & absent$timecat<=32, "one.to.four.days", ifelse(absent$timecat>32 & absent$timecat<=56, "one.week", ifelse(absent$timecat>56 & absent$timecat<=96, "two.week", "three.week")))))
          
#absent$timecat <- as.factor(absent$timecat)
absent2 <- absent #creating an alternate data set for classification models 
absent2$Absenteeism.time.in.hours <- NULL
##Randomizing the data
set.seed(65)
abs_rand<-absent2[sample(nrow(absent)),]
##Turning factors into binary
abs_mm <- as.data.frame(model.matrix(~ . -1, abs_rand[-15])) ##Breaking factors into dummy variables
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
##Normalizing every variable except output
abs_norm <- as.data.frame(lapply(abs_mm, normalize))
absent2 <- cbind(abs_norm, class.ind(absent2$timecat))
```

```{r}
#Separating data into test and train data
library(nnet)
abs_train <- absent2[1:540, ]
abs_test <- absent2[541:740, ]
#Setting up the necessary packages for the ANN model
library(neuralnet)
#Modelling ANN
abs_model <- neuralnet(formula= less.than.day + not.absent + one.to.four.days + one.week + two.week + three.week ~ ., data=abs_train, hidden = 5, threshold = 0.5)
plot(abs_model)
model_results <- compute(abs_model, abs_test[1:42,])
pred_strength <- model_results$net.result
summary(pred_strength)
library(caret)
#so now each of the six outputs have to be converted to a binary result but i'm not sure how to interpret these results and determine a threshold to make a result 0 or 1 pls help!
```

```{r}
## extract classes
classes <- levels(absent$timecat)

######################################################################
# define softmax mapping                                             #
#  Defn: softmax                                                     #
#     g(k) := e^{x_k}/sum_{i = 1}^{n} (e^{x_i})                      #
#  Purpose of softmax is to normalize prediction output of ANN       #
#  to a probability scale                                            #
######################################################################
test_pred<- as.data.frame(pred_strength)
test_pred <- exp(test_pred)
test_pred$sum <- rowSums(test_pred)
test_pred <- test_pred/test_pred$sum
test_pred$sum <- NULL

## make prediction
abs_test$pred <- classes[max.col(test_pred)]
## Confusion matrix
confusionMatrix(as.factor(abs_test$pred), as.factor(absent$timecat[541:740]))
```

```{r}
View(abs_model)
## extract classes
classes <- levels(absent$timecat)

######################################################################
# define softmax mapping                                             #
#  Defn: softmax                                                     #
#     g(k) := e^{x_k}/sum_{i = 1}^{n} (e^{x_i})                      #
#  Purpose of softmax is to normalize prediction output of ANN       #
#  to a probability scale                                            #
######################################################################
test_pred<- as.data.frame(pred_strength)
test_pred <- exp(test_pred)
test_pred$sum <- rowSums(test_pred)
test_pred <- test_pred/test_pred$sum
test_pred$sum <- NULL

## make prediction
abs_test$pred <- classes[max.col(test_pred)]

## Confusion matrix
confusionMatrix(as.factor(abs_test$pred), as.factor(absent2$timecat[541:740]))
```







#Decision Tree
```{r}
## Understanding Decision Trees ----


## Example: Identifying Risky Bank Loans ----
## Step 2: Exploring and preparing the data ----

# create a random sample for training and test data
# use set.seed to use the same random number sequence as the tutorial
set.seed(12345)


# split the data frames
abs_rand$timecat<-as.factor(abs_rand$timecat)
abs_train_tree <- abs_rand[1:640, ]
abs_test_tree  <- abs_rand[641:740, ]

# check the proportion of class variable
prop.table(table(abs_train_tree$timecat))
prop.table(table(abs_test_tree$timecat))

## Step 3: Training a model on the data ----
# build the simplest decision tree
library(C50)
abs_model <- C5.0(abs_train_tree[-14], abs_train_tree$timecat)

# display simple facts about the tree
abs_model

# display detailed information about the tree
summary(abs_model)

#plot(abs_model)

#mod1 <- C5.0(Species ~ ., data = iris)
#plot(mod1)

## Step 4: Evaluating model performance ----
# create a factor vector of predictions on test data
abs_pred <- predict(abs_model, abs_test_tree)

# cross tabulation of predicted versus actual classes
library(gmodels)
CrossTable(abs_test_tree$timecat, abs_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual timecat', 'predicted timecat'))
## Step 5: Improving model performance ----

## Boosting the accuracy of decision trees
# boosted decision tree with 10 trials
abs_boost10 <- C5.0(abs_train_tree[-14], abs_train_tree$timecat,
                       trials = 10)
abs_boost10
summary(abs_boost10)
plot(abs_boost10)
abs_boost_pred10 <- predict(abs_boost10, abs_test_tree)
CrossTable(abs_test_tree$timecat, abs_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual timecat', 'predicted timecat'))
confusionMatrix(abs_test_tree$timecat, abs_boost_pred10)
# boosted decision tree with 100 trials (not shown in text)
abs_boost100 <- C5.0(abs_train_tree[-14], abs_train_tree$timecat,
                        trials = 100)
abs_boost_pred100 <- predict(abs_boost100, abs_test_tree)
CrossTable(abs_test_tree$timecat, abs_boost_pred100,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
confusionMatrix(abs_test_tree$timecat, abs_boost_pred100)
plot(abs_boost100)
## Making some mistakes more costly than others
# create a cost matrix
error_cost <- matrix(c(0, 1, 4, 0,0,1,4,0,0,1,4,0,0,1,4,0,0,1,4,0,0,1,4,0,0,1,4,0,0,1,4,0,0,1,4,0), nrow = 6)
error_cost

# apply the cost matrix to the tree  "Reason for absence" accounts for 62% accurancy, it's the most important factor when estimating how long you will be absent. Don't do classfication, do prediction. 
abs_cost <- C5.0(abs_train_tree[-14], abs_train_tree$timecat,
                          costs = error_cost)
abs_cost_pred <- predict(abs_cost, abs_test_tree)

CrossTable(abs_test_tree$timecat, abs_cost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
confusionMatrix(abs_test_tree$timecat,abs_cost_pred)
plot(abs_cost)
```


#KSVM Model
```{r}
absent <-read.csv("Absenteeism_at_work2.csv")
str(absent)
```

```{r}
##Converting into factors
#absent$ï..ID<- as.factor(absent$ï..ID)
absent$timecat <- as.factor(absent$timecat)
```

```{r}
n <- nrow(absent)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_absent <- absent[tindex,]   # Create training set
test_absent <- absent[-tindex,]   # Create test set
## Construct SVM Model
library(e1071)
svm1 <- svm(timecat~., data=absent, 
          method="C-classification", kernal="vanilladot", 
          gamma=0.1, cost=10)
summary(svm1)
```
```{r}
## Making Predictions
prediction <- predict(svm1, test_absent)
## Display confusion matrix
library(caret)
xtab <- confusionMatrix(as.factor(test_absent$timecat), as.factor(prediction))
xtab
```