---
title: "Project3"
author: "Group 6"
date: "4/2/2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapse: false
    number_sections: true
    theme: cerulean
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(MASS)
library(BART)
library(kernlab)
library(class)
```
# Introduction of the data set
The Absenteesim data set has 21 columns of variables, not all of which may be ready for immediate regression and analysis. Moreover, for the sake of time and memory efficiency, we shall eliminate variables that are not scalable (those that cannot be converted to a numeric scale or ordered sequence), and have too many factor levels.


# Data Import and Data Cleaning
```{r}
setwd("/Users/candicelin/Desktop/TO 414/Absenteeism_at_work_AAA")
absent <-read.csv("Absenteeism_at_work2.csv")

```

First, we tried to run linear models. However, because we have too many variables whereas our data only have 740 rows, most variables in our linear model turned out to be insignificant. therefore we decided to get rid of some variables that are distracting and decreasing model accuracy.For example,we deleted the variable "ID", then we turned Seasons into a binary variable "is_Seas4" because Season4 is the only significant factor among 4 seasons.For similar reasons, we turned Day.of.the.week into a binary variable "is_Thurs". Also, we deleted some insignificant variables: 
"Month of absence","Weight",Education","Transportation expense","Social drinker","Disciplinary.failure".

```{r}
##Converting into factors 
#absent$ï..ID<- as.factor(absent$ï..ID)
absent$Reason.for.absence <- as.factor(absent$Reason.for.absence)
absent$Month.of.absence <- as.factor(absent$Month.of.absence)
absent$Day.of.the.week <- as.factor(absent$Day.of.the.week)
absent$Seasons <- as.factor(absent$Season)
absent$Disciplinary.failure <-as.factor(absent$Disciplinary.failure)
absent$Education <- as.factor(absent$Education)
absent$Social.drinker <-as.factor(absent$Social.drinker)
absent$Social.smoker <-as.factor(absent$Social.smoker)
absent$ï..ID  <- NULL
absent$ID  <- NULL
absent$Work.load.Average.day<-as.numeric(absent$Work.load.Average.day)
absent$Month.of.absence<- NULL
absent$Weight<- NULL
absent$is_Thur<-if_else(absent$Day.of.the.week==5,1,0)
absent$Day.of.the.week<-NULL
absent$is_Seas4<-if_else(absent$Seasons==4,1,0)
absent$Seasons<-NULL
absent$Education<-NULL
absent$Transportation.expense<-NULL
absent$Social.drinker<-NULL
absent$Disciplinary.failure<-NULL
```

## Linear Modeling: Determining Significant Predictors of Absenteesim

Now, let's run linear models on the remaining variables again and see if they give us better $R^2$ value.

```{r}
lm1 <-lm(Absenteeism.time.in.hours ~.,data=absent) %>% stepAIC(trace = FALSE, direction = "forward")
summary(lm1)

##Trying to get interactions
lm2 <- lm(Absenteeism.time.in.hours ~.,data=absent) %>% stepAIC(scope = . ~ .^2 , trace = FALSE , direction = "forward")
summary(lm2)
```

## Explanation on significant factors, getting rid of insignificant factors
* ***Reason for absense***
 +Overall reasons for absence are significant, but some reasons have more of impact than others. I guess that's because some reasons are commonplace or popular while others are more personal and not widely used.

* ***Month of absence***
 +It is not a significant factor, however by common sense, we would expect people to take more leaves during holiday season,such as Christams and New Year Holiday,and take less leaves after that, which is the first quarter of the year.

* ***Day of the Week***
 +Comparatively, people take more leaves in the first few days of the week, one possible explanation is people tend to have "Monday Syndrome", and in the last few workdays(Thursdays and Fridays) they tend to work harder and longer to finish the work in time in order to have a good weekend.

* ***Seasons***
 +Similar to Month of absense. We can see people take longer leaves during season 2,3,4 than season 1, I guess that's because in season 1 most people just come back from Christmas and New Year Holiday, and there are less holidays in season 1.

* ***Transportation expense***
 +We expected that people who need more transportation expenses tend to have more absense. Interestingly, the coefficient is postive but really small, also it is not significant. I guess this is because most people won't consider transportation expenses when they are thinking if they want to go to work today (since you can make more money than transportation expenses if you work)

* ***Distance from Residence to Work***
 +The result is kind of counterintuitive since we expectd that people who liver closer to the company will show up more compared to people who don't. Though it is not significant, the relationship is negative. Maybe it is becuase people who liver closer tend to procrastinate and leave home later than people who live farther away and get up earlier.

* ***Service time***
 +Intuitively, people who do more volunteer work would of course ask for more leaves than people who don't.
* ***Age***
 +Older people may ask for more leaves due to health issues, or because they are more senior so the legwork can be handed to juniors and they can leave earlier.

* ***Work load Average day***
 +The coefficient is negative, which is straigtforward because the more workload you have the less likely you will take a break.

* ***Hit target***
 +If you have hit the target of your manager/your team, you can be rewarded with more breaks.

* ***Son***
 +It is pretty straightforward that more kids means people need more commitment to the family and more parental leaves, therefore the relationship is negative.

* ***Pet***
 +We expected it to be similar to "Son", however the relationship is negative. One possible interpreation is that people who have pets are mostly young people who don't have kids. Also, pets need less dedication that kids(since you don't need to cook for pets and tutor them)

* ***Weight, Height, Body mass index***
 +We expected a healthy body would save people from taking sick leaves. Weight and height are very insignificant since people with different heights have different healthy weights. BMI index is also significant since a good BMI is between 18.5-24.9——neither a too high or too low BMI index is good.

## Data Cleaning again, randomizing and normalizing data

Now, we need to further clean, randomize and normalize our data, in preparation for KNN,ANN, Decision Tree models,etc.

```{r}
##Building categories
absent$timecat <- absent$Absenteeism.time.in.hours
absent$timecat <- ifelse(absent$timecat <= 7, "less.than.day", ifelse (absent$timecat<=35, "less.than.one.week", "more.than.one.week"))
more<-subset(absent, absent$timecat=="more.than.one.week")
absent <- rbind(absent,more,more,more,more)          
absent$timecat <- as.factor(absent$timecat)
absent$Absenteeism.time.in.hours <- NULL


##Randomizing the data
set.seed(65)
absent_r<-absent[sample(nrow(absent)),]

##Turning factors into binary
absent_mm <- as.data.frame(model.matrix(~ . -1, absent_r[-14])) 


normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

##Normalizing every variable except output
absent_n <- as.data.frame(lapply(absent_mm, normalize))
```

* Struggles and trials we have been through in this stage:
We struggled a lot with creating categories for Absenteeism in hours. Our response variable was absenteeism time in hours and after some data exploration we noticed that the distribution of hours was not continuous. It was discrete and mostly in increments of 8 hours after surpassing a day of absence. So, we  decided to make this a factor instead since we believed that would provide more information. For example, stating that an employee would be absent for 1-3 days is more intuitive to understand than 30 hours. 
First we made 6 categoriess("not.absent","less.than.one.day","one.to.four.days","four.days.to.one.week","one.to.two.weeks","more.than.two.weeks") but we ran into the issue of a skewed distribution among the classes, hence we had very low Accuracy and Kappa. We noticed that the large majority of people were only absent for a couple hours in a day, the second largest majority was between 1-5 days. After some trails, we settled on a factor of 3 levels- "less.than.day", "less.than.one.week", and "more.than.one.week".

Furthermore,we decided to add more data rows(from 740 to 828) so that we can have more evenly distributed data.(so that we don't have a test data set with 0 data in"more.than.one.week") 

# Classification
Below are the models we made in order to find out the best model in predicting roughly how many days/weeks an employee will be absent.
##1. KNN Model
We now build a KNN model with the randomized and normalized data set and significant variables that we decided to keep.
```{r}
##Getting my own absent from the above
absent_SP <- cbind(absent_n, absent$timecat)

##Rebulding the test and train data
abs_train <- absent_SP[1:728, -41]
abs_test <- absent_SP[729:828,-41]
abs_train_label <- absent_SP[1:728, 41]
abs_test_label <- absent_SP[729:828,41]

##building KNN model
KNN_MODEL <- knn(train = abs_train, test = abs_test,
                      cl = abs_train_label, k=20)

confusionMatrix(as.factor(KNN_MODEL),as.factor(abs_test_label))
```

Interpretation:

## 2. ANN Model
Now we build ANN models with 2/5/8 neurons.
```{r}
absam<-absent_SP
#View(absam)
catint<-class.ind(as.factor(absam$`absent$timecat`))
train<-cbind(absam[,-41],catint)

n<-names(train)
f <- as.formula(paste("less.than.day + less.than.one.week + more.than.one.week ~", paste(n[!n %in% c("less.than.day", "less.than.one.week", "more.than.one.week" )], collapse = " + ")))


library(neuralnet)
nn <- neuralnet(f,
                data = train,
                hidden = c(8,5,2),
                rep = 3,
                act.fct = "logistic",
                linear.output = FALSE,
                lifesign = "minimal",
                threshold = 0.5)

#plot(nn)

pr.nn <- compute(nn, train[, 1:40])

# Extract results
pr.nn_ <- pr.nn$net.result



original_values <- max.col(train[, 41:43]) #takes column number with the maximum value

pr.nn_2 <- max.col(pr.nn_)
mean(pr.nn_2 == original_values)


confusionMatrix(as.factor(pr.nn_2),as.factor(original_values))
```

Interpretation:

## 3. Decision Tree, Boosted Decision Tree with 10 trials/100 trials/a cost matrix
Now, we build three different Decision Tree Models and graphs to see which variable accounts for the most Accuracy and how good Decision Tree Models are when predicitng Abseenteeism.
```{r}
set.seed(12345)


# split the data frames
absent_r$timecat<-as.factor(absent_r$timecat)
abs_train_tree <- absent_r[1:728, ]
abs_test_tree  <- absent_r[729:828, ]

# check the proportion of class variable
prop.table(table(abs_train_tree$timecat))
prop.table(table(abs_test_tree$timecat))

# build the simplest decision tree
library(C50)
abs_model <- C5.0(abs_train_tree[-14], abs_train_tree$timecat)

# display simple facts about the tree
abs_model

# display detailed information about the tree
summary(abs_model)

plot(abs_model)
#Reason6: Disease for circulatory system


# create a factor vector of predictions on test data
abs_pred <- predict(abs_model, abs_test_tree)

# cross tabulation of predicted versus actual classes
library(gmodels)
CrossTable(abs_test_tree$timecat, abs_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual timecat', 'predicted timecat'))
confusionMatrix(abs_test_tree$timecat, abs_pred)

```

The above Decision Tree model gives a really high accuracy (~90%) and a high Kappa (~83%). Let's see if boosting can further improve the accuracy and Kappa.

```{r}
## Boosting the accuracy of decision trees
# boosted decision tree with 10 trials
abs_boost10 <- C5.0(abs_train_tree[-14], abs_train_tree$timecat,
                       trials = 10)
summary(abs_boost10)
plot(abs_boost10)
abs_boost_pred10 <- predict(abs_boost10, abs_test_tree)
CrossTable(abs_test_tree$timecat, abs_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual timecat', 'predicted timecat'))
confusionMatrix(abs_test_tree$timecat, abs_boost_pred10)
```

Boosted decision tree with 10 trials increased Accuracy to 91% and Kappa to 85%.
Next, we increase the trial numbers to 100 to see if this can further improve Accuracy and Kappa.
```{r}
# boosted decision tree with 100 trials (not shown in text)
abs_boost100 <- C5.0(abs_train_tree[-14], abs_train_tree$timecat,
                        trials = 100)
abs_boost_pred100 <- predict(abs_boost100, abs_test_tree)
CrossTable(abs_test_tree$timecat, abs_boost_pred100,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
confusionMatrix(abs_test_tree$timecat, abs_boost_pred100)
```

Surprisingly, boosted decision tree with 100 trials does not improve the model much.
Next, we tried to create a cost matrix to see if it can improve Accuracy and Kappa.
```{r}
## Making some mistakes more costly than others
# create a cost matrix
error_cost <- matrix(c(0,4,4,4,0,4,4,4,0), nrow = 3)


# apply the cost matrix to the tree  "Reason for absence" accounts for 62% accurancy, it's the most important factor when estimating how long you will be absent. Don't do classfication, do prediction. 
abs_cost <- C5.0(abs_train_tree[-14 ], abs_train_tree$timecat,
                          costs = error_cost)
abs_cost_pred <- predict(abs_cost, abs_test_tree)

CrossTable(abs_test_tree$timecat, abs_cost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
confusionMatrix(abs_test_tree$timecat,abs_cost_pred)

```
Decision tree with a cost matrix does not improve the model much, either.
Therefore, the best model is the boosted decision tree with 10 trials.

## KSVM Model
Below, Absenteeism is predicted using an SVM classifier with a linear kernel using the significant features found above as the predictors. 
```{r}
library(kernlab)
ksvm_model1 <- ksvm(timecat~ ., data = abs_train_tree,
                          kernel = "vanilladot")

# look at basic information about the model
ksvm_model1

# predictions on testing dataset
ksvm_predictions <- predict(ksvm_model1, abs_test_tree)
ksvm_predictions <- as.factor(ksvm_predictions)
confusionMatrix(ksvm_predictions,abs_test_tree$timecat)

```
We tried the KSVM model with various kerns. After doing a confusion matrix for both models, we found the KSVM model with rbf kernel has a more accurate prediction rate.

```{r}
## Trying a different kernel
ksvm_model_rbf <- ksvm(timecat~ ., data = abs_train_tree, kernel = "rbfdot")
ksvm_predictions_rbf <- predict(ksvm_model_rbf, abs_test_tree)
ksvm_predictions2 <- predict(ksvm_model_rbf, abs_test_tree)
ksvm_predictions2 <- as.factor(ksvm_predictions2)
confusionMatrix(ksvm_predictions2,abs_test_tree$timecat)

```


# SVM Model

```{r}

n <- nrow(absent)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_absent <- absent_r[tindex,]   # Create training set
test_absent <- absent_r[-tindex,]   # Create test set


## Construct SVM Model
library(e1071)
svm1 <- svm(timecat~., data=absent, 
          method="C-classification", kernal="vanilladot", 
          gamma=0.1, cost=10)

## Making Predictions
prediction <- predict(svm1, test_absent)
## Display confusion matrix
library(caret)
xtab <- confusionMatrix(as.factor(test_absent$timecat), as.factor(prediction))
xtab
```
We consider the model above to be "good" in the sense that the $p$- value is very small, and the accuracy is high (at least better than chance, and as close as possible to 1).
Linear KSVM model gives 77% accuracy and 59% Kappa;KSVM model with rbf kernal gives 95% accuracy and 91% Kappa.After using the above models to predict Absenteeism, we found that the Decision Tree model and KSVM models are more accurate. 

From the decision tree graph, we can see Reason for Absense is the most important facotor when classifying absent hours, let's make a random forest model and see if it also tells us the same conclusion.

# 4. Random Forest + Explainer
```{r}
library(randomForest)
# Training with Random forest model
forest <- randomForest(timecat ~. , data=train_absent, localImp = TRUE)
forest
```


```{r}
print(forest, digits=3)
# Predict the testing set with the trained model
predictions2 <- predict(forest, test_absent, type = "class")

# Accuracy and other metrics
confusionMatrix(predictions2, test_absent$timecat)
```

```{r}
library(randomForestExplainer)
min_depth_frame <- min_depth_distribution(forest)
save(min_depth_frame, file = "min_depth_frame.rda")
load("min_depth_frame.rda")
head(min_depth_frame, n = 10)

# plot_min_depth_distribution(forest) # gives the same result as below but takes longer
plot_min_depth_distribution(min_depth_frame)
plot_min_depth_distribution(min_depth_frame, mean_sample = "relevant_trees", k = 15)

importance_frame <- measure_importance(forest)
save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
importance_frame
```

```{r}
explain_forest(forest, interactions=TRUE, data=train_absent)
```

Interpretation:



# Prediction 

## ANN model
```{r}
setwd("/Users/candicelin/Desktop/TO 414/Absenteeism_at_work_AAA")
absentANN <-read.csv("Absenteeism_at_work2.csv")
str(absentANN)
#absentANN$Reason.for.absence <- as.factor(absentANN$Reason.for.absence)
#absentANN$Month.of.absence <- as.factor(absentANN$Month.of.absence)
#absentANN$Day.of.the.week <- as.factor(absentANN$Day.of.the.week)
#absentANN$Seasons <- as.factor(absentANN$Season)
absentANN$Disciplinary.failure <-as.factor(absentANN$Disciplinary.failure)
#absentANN$Education <- as.factor(absentANN$Education)
#absentANN$Social.drinker <-as.factor(absentANN$Social.drinker)
#absentANN$Social.smoker <-as.factor(absentANN$Social.smoker)
absentANN$ID  <- NULL
absentANN$Work.load.Average.day<-as.numeric(absentANN$Work.load.Average.day)
absentANN$Month.of.absence<- NULL
absentANN$Weight<- NULL
absentANN$is_Thur<-if_else(absentANN$Day.of.the.week==5,1,0)
absentANN$Day.of.the.week<-NULL
absentANN$is_Seas4<-if_else(absentANN$Seasons==4,1,0)
absentANN$Seasons<-NULL
#absent$is_Educ3<-if_else(absent$Education==3,1,0) Once I do this, Seasons and Education3 no longer significant
absent$Education<-NULL
absentANN$Transportation.expense<-NULL
absentANN$Social.drinker<-NULL
absentANN$Disciplinary.failure<-NULL


# apply normalization to entire data frame
absent_ANN <- as.data.frame(lapply(absentANN, normalize))

# confirm that the range is now between zero and one
summary(absent_ANN$Absenteeism.time.in.hours)

# compared to the original minimum and maximum
summary(absentANN$Absenteeism.time.in.hours)

# create training and test data
absent_ANN_train <- absent_ANN[1:540, ]
absent_ANN_test <- absent_ANN[541:740, ]

## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)

# simple ANN with only a single hidden neuron
ANN_model <- neuralnet(Absenteeism.time.in.hours~.,
                              data = absent_ANN)


# visualize the network topology
plot(ANN_model)

## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(ANN_model, absent_ANN_test)
# obtain predicted strength values
predicted_strength <- model_results$net.result
# examine the correlation between predicted and actual values
cor(predicted_strength, absent_ANN_test$Absenteeism.time.in.hours)

## Step 5: Improving model performance ----
# a more complex neural network topology with  hidden neurons
ANN_model2 <- neuralnet(Absenteeism.time.in.hours ~ ., data=absent_ANN,hidden = 15)

# plot the network
#plot(ANN_model2)
# evaluate the results as we did before
model_results2 <- compute(ANN_model2, absent_ANN_test)
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, absent_ANN_test$Absenteeism.time.in.hours)


```
With 15 neurons, we get a pretty high accruacy, ~88%. From the result, we can conclude that the variables that we keep are doing a good job in predicting Absenteeism.time.in.hours.


# Remarks about Business values and potential:

Based on Reasons for absence that an employee gives, where he/she lives, average social service time he/she spends every year,age,work load,and the day of the week/the month/the season when he/she askes for a leave, HR can predict how many hours an employee will be absent on the job, therefore it helps on makeing decisions on hiring employees,estimating new hire headcounts, altering healthcare plans,employee relocation, regulations and contract benefits.


